* overview
- we use kubernetes api objects to describe cluster's desired state
- what applications to run
- what container image they use
- no of raplicas
- network and disk/cpu usage

- kubernetes performs variety of tasks automatically
- starting/restarting containers
- scaling no of replicas

- kubernetes master- three processes that run on a single node
- kube-apiserver
- kube-controller-manager
- kube-scheduler

- other/worker node - two processes on each node
- kubelet: communicates with the master
- kube-proxy: proxy network that reflects networking services on each node

* kubernetes-objects
- basic objects
- pod
- service
- volume
- namespace

- high level abstractions: controllers
- based on basic objects
- provides additional functionality

- controllers
- replicaset
- deployment
- statefulset
- daemonset
- job

* kubernetes control plane
- govern how kubernetes communicates with the cluster
- maintains record of all objects
- runs continuous loop to manage those objects state
- responses to change in the claster: work to make actual state to match desired state for all objects


*** kubernetes master
- maintains desired state of the cluster

*** kubernetes nodes
- machines (vm/physical server)
- run apps
- managed my master

* [[https://kubernetes.io/docs/concepts/overview/components][kubernetes components]]
** master components
*** kube-apiserver
- exposes the kubernetes api
- front-end for kubernetes control plane

*** etcd
- key-value store: storeage for all cluster data

*** kube-scheduler
- watches newly created pods
- assigns nodes to them

*** kube-controller-manager
- runs controller(control loop: watches the state of the cluster, works to move current state to desired state)
- each controller: separate process, single binary, runs as signle process
- node controller: notice and responds when nodes go down
- replication controller: maintains the correct number of pods
- endpoints controller: populates the endpoints objects(joins services and pods)
- service account and token controller: create accounts and api access tokens for new namespace

*** cloud controller manager
- runs controller that interacts with underlying cloud providers
- controllers:
- node controllers: checks the cloud provider to determine if a node has been deleted
- router controllers: sets up routes in underlying cloud infra
- service controller: create/update/delete cloud provider node balancer
- volume controller: attach/mount/interacts with cloud provider volumes

** node components
- runs on every nodes
- maintains running pods

*** kubelet
- makes sure that containers are running in a pod
- pods are running and healthy
- doesn't manage containers

*** kube-proxy
*** container runtime
* [[https://kubernetes.io/docs/concepts/overview/kubernetes-api/][kubernetes api]]
* [[https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/][working with kubernetes objects]]
** overview
- persistent entities in the kubernetes system
- kubernetes uses these entities to represent state of the cluster
  - what apps are running, on which nodes
  - resources available to the apps
  - policies around how those apps behave(restart policies, upgrade, fault-tolerance)
- once objects are created, kubernetes works to ensure that objects remain exist

*** object spec and status
 - every object includes two nested object fields that govern the object's configuration: object spec and object status
 - object spec
   - one must provide
   - describes desired state for the object
 - object status
   - actual state of the object
   - supplied and updated by kubernetes
   - kubernetes control plane manages objects actual state to match desired state
** names
** namespaces
- kubernetes supports multiple vertical clusters backed by same physical server
- these virtual clusters are namespaces

*** when to use multiple namespaces
- many users spread across multiple teams/projects
- divide cluster resources between multiple users

*** viewing namespaces
#+BEGIN_SRC shebang
$ kubectl get namespaces
NAME          STATUS    AGE
default       Active    1d
kube-system   Active    1d
kube-public   Active    1d
#+END_SRC

- three initial namespaces
  - default: default namespaces
  - kube-system: namespace for objects created by kubernetes system
  - kube-public: readable by all users
  - 
** labels and selectors
*** labels
- key-value pairs that are attached to an object
- identify attributes of an object that are meaningful to users
- organize and select subset of objects
- labels can be attached on creation/modified later
- 

*** selectors
- many objects  have same lables
- user can identify a set of objects via label selector
- empty label selector selects every object
- null label selector selects no object
- two types of selectors: equality based, set based
  - equality based: filter by keys and values
#+BEGIN_SRC yaml
apiVersion: v1
kind: pod
metadata:
  name: cuda-test
spec:
  containers:
  - name: cuda-test
    image: "k8s.gcr.io/cuda-vector-add:v0.1"
    resources:
      limits:
        nvidia.com/gpu: 1
  nodeSelector:
    accelerator: nvidia-tesla-p100
#+END_SRC
  - set based: allows filtering keys according to a set of values
#+BEGIN_SRC
environment in (production, qa)
tier notin (frontend, backend)
partion
!partition
#+END_SRC

*** watch and list filtering
#+BEGIN_SRC shellbang
kubectl get pods -l environment=production,tier=frontend
kubectl get pods -l 'environment in (production), tier in(frontend)'
kubectl get pods -l 'environment in (production, qa)'
kubectl get pods -l 'environment,environment notin (frontend)'
#+END_SRC

#+BEGIN_SRC yaml
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}
#+END_SRC
** annotations
- non-identifying metadata
- one can use either label or annotation
- labels are used to select and find collection of objects that satisfy certain conditions
- annotations are not used to identify and select objects
- build, release, image information like timestamp, git branch, pr number, hash

** field selector
- select kubernetes objects based on value of one or more resource fields
$ kubectl get pods --field-sellector status.phase=Running
* [[https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/][kubernetes object management]]
* kubernetes architecture 
** nodes
- worker machines
- vm/physical machine
- managed by master
- runs containers
- includes
  - container runtime
  - kubelet
  - kubeproxy
  

*** node controller
- master component
- manages various aspects of nodes
- assigns CIDR block
- update list of nodes
- monitor nodes' health
- when node becomes unhealthy, it asks the cloud provider if it is still available, if not deletes it from list of nodes

** master-node communication
*** cluster to master
- communication path terminate at apiserver(other master components are not exposed)
- apiserver listens at HTTPS 443 port
- nodes contain public root certificate of the cluster
- when pods are created, 

*** master to cluster
**** apiserver to kubelet
used for:
- fetching logs for pods
- attaching to running pods
- providing kubelet's port-forwarding functionality
- 

**** apiserver to nodes/pods/services
- HTTP connection
- not authenticated/encrypted
- not confurrently safe
** cloud controller manager
- design based on plugin mechanism
- allows cloud providers integrate with kubernetes using plugins
- 

*** without cloud controller manager
#+DOWNLOADED: /tmp/screenshot.png @ 2018-11-30 12:53:16
[[file:kubernetes%20architecture/screenshot_2018-11-30_12-53-16.png]]

*** with cloud controller manager
  
#+DOWNLOADED: /tmp/screenshot.png @ 2018-11-30 12:47:37
[[file:kubernetes%20architecture/screenshot_2018-11-30_12-47-37.png]]



*** cloud controller manager runs:
- node controller
- router controller
- service controller
- persistent volume label controller
**** node controller
- initialize a node with cloud specific zone/region label
- initialize a node with cloud specifi detail, type and size
- obtain node's network adress and hotname
- check if node is deleted from cloud, if yes then delete the kubernetes node object
**** route  controllers
- configures routes so containers on different nodes in the cluster can communicate with each other
- only applicable for GCE clusters
**** service controller
- listens to service create, update, delete events
- configures cloud load balancers
- 

**** persistent volume labels controller
- applies labels on volumes when crated
- removes the need of users manually set the labels
- these labels are essential for scheduling pods, as they only work within the region/zone that they are in, so pods need to be in same region
- 

** kubelet
- node controller contains cloud dependent functionality of the kubelet
- before, kubelet was responsible for cloud-specific details(ip address, region/zone labels, instant type information)
- now, the initialization operations are done in ccm
- now, kubelet initialize a node without cloud specific information
- new nodes are unschedulable until ccm initializes the node with cloud specific informations
* [[https://kubernetes.io/docs/concepts/containers/][containers]]
* [[https://kubernetes.io/docs/concepts/workloads/pods/][pods]]
** overview
*** understanding pods
**** overview
     - smallest deployable object
     - basic building block
     - smallest and simplest unit
     - represents a running process
     - pods encapsulate
       - application containers(or multiple containers)
       - storage
       - unique ip
       - options that govern how the containner(s) should run
     - containers are tightly coupled and share resource
     - single instance of an application
       - if scaled horizontally, multiple pods, one for each instance
       - replicated pods created and managed by controller

**** pods that run single containers
     - most common use case
     - pod as a wrapper around single container
     - kubernetes manages pod rather than conntainers directly

**** pods that run multiple containers
     - encapsulate an application that composed of multiple co-located containers that are tightly coupled and need to share resource
     - form a single cohesive unit of service

**** how pods manage multiple containers
     - multiple cooperating processes as containers form a cohesive unit of service
     - containers are automatically co-located and co-scheduled on same node
     - share resource, dependencies
     - communicate with each other
     - coordinate when and how to terminate
     - advanced use case
     - only use when containers are tightly coupled

**** shared networking
     - each pod is assigned a unique ip address
     - every container in a pod shares the network namespace including ip addresses and ports
     - containers can communicate with each other using *localhost*

**** shared storage
     - pod specify a set of shared storage volumes
     - all containers can access the shared volumes
     - allows persistent data in a pod
*** working with pods
    - you'll rarely create individual pods directly
      - pods are designed as relatively short lived, disposable entities
      - when created, it is scheduled to run on a node
      - remains on that node until process terminates, then pod is deleted
      - do not self-heal
      - pods get deleted
        - if node fails
        - scheduling operations fails
        - lack of resource
        - node maintainance
    - controller handles the work managing the disposable pod instances
    - so, it is far more common to manage pods using a controller
*** pods and controllers
    - create and manage multiple pods
    - handles replication, rollout and self healing capabilities
    - if node fails, controller might automatically replace the pod by scheduling an identical replacement on different node
    - example of controllers that contain one or more pors
      - deployment
      - statefulset
      - daemonset
*** pod template
    - pod specifications which are included in other objects such as replication controller, jobs, daemonset
    - uses pod templates to make actual pods

    #+BEGIN_SRC yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: myapp-container
      image: busybox
      command: ['sh', '-c', 'echo hello kubernetes! && sleep 3600']
    #+END_SRC

    - after creation, pods can't be updated
    - new template has no direct effect on pods already created
    - but pods created by controllers can be updated directly
** pods
   - pods are isolated by linux namespaces, cgroups
   - within pod, individual containers are furher sub-isolationed
   - containers within a pod share an ip addresses and port space
   - find each other via localhost
   - communicate with each other using standard inter process communications like semaphores or shared memory
   - containers in different pods have different ip addresses
   - can't communicate by IPC
   - they communicate via pod ip address
   - 

** termination of pods
   - pods are gracefully terminated
   - when users request deleteion of pod, system records the intended grace period and TERM signal is sent to main process of each container
   - once grace period has expired, the KILL signal is send to those processes
   - pods are then deleted from API server
** container probes
- probe is a diagnostic performed periodically by kubelet
- kubelet calls a handler implemented by the container
- 3 types of handlers
  - runs specific command inside the container. diagnostic is successful if command exists with status code 0
  - performs TCP check against container's IP on spefcified port. diagnostic successful if port is open
  - performs HTTP GET request against container's IP address on specified port and path. Diagnostic successful if response code betwen [200, 400)
- optionally performs two types of probles
  - livenessProbe:
    - indicates whether the container is running
    - if fails, kubelet kills the container
    - container is then subjected to its restart policy
    - if container does not provide a liveness probe, default state is success
  - readinessProbe:
    - indicates whether the container is ready to service requests
    - if fails, the endpoint controller removes pod's ip address from all services that match the pod
    - if container does not provide a readinessProbe, default state is success
    
*** when to use probes
- if container crashes on its own whenever it encounters an issue or becomes unhealty, no need for liveness probe
- if you want to send traffic to pod when a probe succeds, specify readiness probe
- the pod will start without recieving any traffic, only start recieiving after probe starts succeeding
- if container needs to load large data, configuring files, specify readiness probe
** init containers
** pod presets
*** overview
- inject additional runtime requirements into a pod at creation time
- use label selectors to specify the pods to which the preset applies
- allows not to have explicitly provide all information for every pod
- authors of pod templates consuming a specific service do not need to konow all the details of that service

*** how it works
- admission controller(PodPreset) which when enabled, applies pod presets to incoming pod creation request
- when pod creation request occures, system does
  - retrieve all PodPrestets
  - checks if label selectors of any PodPreset matches the labels of the pod
  - attempts to merge various resources defined by podpreset
  - on error, through event, and create the pod without injecting any resource from podpreset
  - annotate the the resulting modified pod spec to indicate that it has been modified by podpreset
- when podpreset applies to one or more pods
  - modifies the pod specs
  - for Env EnvFrom VolumeMounts, modifies the container spec for all containers
  - for Volume, modifies the pod spec
** discuptons

* [[https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/][replica set]]
- next generation replication controller
- only difference between replicaset and replication controller is selector support
- replicaset supppports the new set based selector
- replication controller only supports equality based selector


** how to use replicaset
- most kubectl commands that supports replication controller also supports replicasets
- only exception is rolling-update command
- for rolling update functionality, one should use deployments instead
- replicasets can be used independently
- mainly used by deployments


** when to use replicaset
- replicaset ensures that spcified number of pod replicas are running at any given time
- we should use deployments instead of directly using replicasets
* [[https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/][replicationcontroller]]
- ensures that specified number of pod replicas are running at any one time
- makes sure that pods are always up and available

** how replicationcontroller works
- if too many pods, terminates extra pods
- if too few pods, creates more pods
- pods managed by replicationcontroller are automatically replaced if they fail, deleted or terminated
* [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/][deployments]]
- controller that provides declarative udpates for pods and replicasets
- you describe a desired state in deployment, and deployment controller changes tha actual state to the desired 
* [[https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/][statefulset]]
- used to manage stateful applications
- manages the deployment and scaling of a set of pods
- provides gurantee about ordering and uniquness of these pods
- like deployment, manages pods that are based on an identical container spec
- unlike deployments, maintains a sticky identity for each of their pods
- these pods are of same specs, but are not interchangeable
- each has a persistent identifier
- that is maintained across any rescheduling


** using statefulsets
- stable unique network identifier
- stable, persistent storage
- ordered, graceful deployment and scaling
- ordered, automated rolling updates
- if application doesn't require any stable identifiers or ordered deployment, deletion or scaling, you should deploy it with deployments

** limitaions
- deleting/scaling down doesn't delete the volumes associated with the stateful
- it is because of data safety
- do not guarantee on the termination of pods when statefulset is deleted
- to achive ordered and graceful termination of the pods, it is possible ot scale down to 0 prior to deletion
* [[https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/][daemonset]]
- ensures that all of some nodes run a copy of a pod
- as nodes are added, pods are added to them
- as nodes are deleted, pods are deleted
** use cases
- running cluster storage daemon
- running log collection daemon
- running node monitoring daemon on every node
* [[https://kubernetes.io/docs/concepts/services-networking/service/][services]]
** overview
- pods are mortal
- pods get own ip, but they can't be relied upon to be stable over time
- if some st of pods(backend) provides functionality to other pods(frontend), how do those frontends find out and keep track of which backends are in that set
- service: defines a logical set of pods and a policy by which to access them
- the set of pods targeted by a service is determined by a label selector
- image processing backend with three replicas
- frontend do not care which backend they use
- backend pods can change, but frontend clients should not be aware of them
- for kubernetes native apps, kubernetws offers endpoints api that is updated whenever the set of pod in a service change
- for non native apps, kubernets offers virtual ip based bridge to services which redirects to the backend pods

** defining a service
#+BEGIN_SRC yaml
kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
#+END_SRC
- it'll create a service named ~my-service~
- it targets 9376 on any pod with ~app=myapp~ label
- service will have an IP (clusterIP)
- maps incoming port to any targetPort
- by default, targetPort is same as port
- targetPort can be string, the name of a port in the pods
- so you can change the port number that pod exposes without breaking service

- kube-proxy watches the kubernets master for addition and removal of service
- for each service, it opens a port(randomly chosen) on local node
- any connecton to this(proxy) port will be proxied to the one of service's backend pods
- it installs iptable rules which caputre traffic to the service's clsuterIP and port
- then redicects that traffic to the proxy port which proxies the backend pods
** service without selectors
- service abstracts access to pods
- they can also abstract other bacnekds
  - you want to point your service to a service in another namespace
  - you are migrating your workload to kubernetes and some of your backends run outside of kubernetess
  - you want to have external database cluster in production, but in test you use your own database

#+BEGIN_SRC yaml
kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
#+END_SRC
- service has no selector
- corresponding endpoints object will not be created
- you can manually map the service to specific endpoints

** headless services
- sometimes you don't need/want loadbalanceing and single service IP
- headless service in that case
- "none" in clusterIP field
- cluster IP not allocated
- kube-proxy doesn't handle these services
- no loadbalancing or proxying done by the platform for them

*** with selectors
- headless services with selectors, endpoints controller creates endpoints records
- modifies the DNS configureation to recotd addresses that point directoly to the pods

*** without selectors
- endpoint controller don't create endponts records

** publishing services:  service types
- ClusterIP
- NodePort
- LoadBalancer
- ExternalName

*** ClusterIP
- exposes service on cluster-internal IP
- service only reachable from within the cluster
- default service type

*** NodePort
- exposes service on each node's ip at a specific port
- a clusterIP service, to which the nodeport service will route, is automatically created
- you can contact nodeport service from outside of the cluster, <NODE_IP>:<NODE_PORT>

*** LoadBalancer
- exposes service externally using cloud provider's load balancer
- nodeport and clusterIP service to which external load balancer will route are automatically created
- for cloud providers that support external load balancers, it'll provision a load balancer for your service
- traffic from external load balancer will be directed at the backend pods

*** ExternalName
- maps the service to the contents of the externalName field
- no proxying of any kind is set up
- maps a service to a DNS name

#+BEGIN_SRC yaml
kind: Service
apiVersion: v1
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
#+END_SRC

- when looking up the host =my-service.prod.svc.cluster.local= dns service will return =my.database.example.com=
- redirection happens in DNS label rather than proxying or forwarding

* [[https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/][DNS for services and pods]]
** overview
- kubernets schedules DNS pod and service on the cluster
- configures the kubelet to tell containers to use the DNS service's IP to resolve DNS names
- every service is assigned a DNS name
- service name =foo= in namespace =bar=
- pod running in namespace =bar= can look up this service by =foo=
- pod running in namespace =xyz= can look up by =foo.bar=

** services
*** A record
- normal (not headless) services are assigned a DNS A record of the form =my-svc.my-namespace.svc.cluster.local=
- this resolves to the clusterIP of the service
- headless services are also assigned DNS A record of the form =my-svc.my-namespace.svc.cluster.local=
- this resolves to the set of IPs of the pods selected by the service

*** SRV record
- SRV records are created for named ports that are part of normal/headless service
- for named port, =_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local=
- this resolves to =my-svc.my-namespace.svc.cluster.local=
- for headless service, this resolves to a set of IPs of the pod
- contains the port number and the domain name of the pod =auto-generated-name.my-svc.my-namespace.svc.cluster.local=

** pods
*** A record
- pods are assigned DNS A record =pod-ip-address.my-namespace.pod.cluster.local=
- pod ip =1.2.3.4= in namespace =default=, dns =1-2-3-4.default.pod.cluster.local=

*** pod's hostname and subdomain fields
- when pods are created, hostname is =metadata.name=
- optional =hostname= field
- when specified, takes precedence over pod's name to be the hostname
- also have =subdomain= field
- pos with =nostname:foo= and =subdomain:bar= in namespace =my-namespace=, DNS =foo.bar.my-namespace.svc.cluser.local=

* [[https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/][connecting applications with services]]
** overview
- docker use host private networking
- containers can talk to each other only if they are on the same machine
- to commincate across nodes, there must be allocated ports on the machine's own ip address
- they are then forwarded or proxied to the containers
- so, containers must co-ordinate which ports they use, or ports must be allocated dynamically
- coordinating ports is very difficult to do at scale
- exposes users to cluster-level issues outside of their control
- kubernetes assumes that pods can communate with other pods, regardless of which host they land on
- every pod is given own cluster-private-ip
- no need to explicitly create links between pods or map host ports to container ports
- containers within a pod, can reach each other's ports on localhost
- all opds in a cluster can see each other wihtout NAT

** exposing pods to cluster
#+BEGIN_SRC yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx
spec:
  selector:
    matchLabels:
      run: my-nginx
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerPort: 80
#+END_SRC

#+BEGIN_SRC shellbang
$ kubectl get pods -l run=my-nginx -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP            NODE
my-nginx-77f56b88c8-cj2fr   1/1     Running   0          14s   172.17.0.20   node-1
my-nginx-77f56b88c8-ptt8t   1/1     Running   0          14s   172.17.0.19   node-2
#+END_SRC

- can ssh into any node and curl both IPs
- kubectl exec into any pod and curl into both IPs

** creating service
- when node dies, pod dies with it, deployment creates new pods with different IPs
- this is the problem that servicce solves
- service defines a logical set of pods that all provide same functionality
- when created, each service is assigned a unique IP address(clusterIP)
- this is tied to the lifespan of the service, will not change while service is alive
- pods can be configured to talk to the service
- communication to the service will be automatically load-balanced out to some opd that is a member of the service

#+BEGIN_SRC yaml
apiVersion: v1
kind: Service
metadata:
  name: my-nginx
  labels:
    run: my-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    containerPort: 80
  selector:
    run: my-nginx
#+END_SRC

- it'll create a service which targets TCP port 80 on any pod with the  ~run: my-nginx~ label
- targetPort: the port the container accepts traffic on
- port: the service port, which can be any port that other pods use to access the service
- you can curl the nginx service on ~<CLUSTER_IP>:<PORT>~ from any node/pod


- a service is backed by a group of pods
- these pods are exposed through endpoints
- when pod dies, it is automaticall yremoved from the endpoints
- new pods matching service's selector will be automatically added to endpoints

#+BEGIN_SRC shellbang
$ kubectl get pods -l run=my-nginx -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP            NODE
my-nginx-77f56b88c8-cqqsg   1/1     Running   0          1h    172.17.0.19   minikube
my-nginx-77f56b88c8-v22kz   1/1     Running   0          1h    172.17.0.20   minikube

$ kubectl get endpoints my-service
NAME         ENDPOINTS                       AGE
my-service   172.17.0.19:80,172.17.0.20:80   1h
#+END_SRC

** accessing service
two primary modes for finding service
- environment variable (works out of the box)
- DNS (requires CoreDNS addon)


*** environment variable
- when pod runs on a node, kubelet adds a set of env vars for each active service
- this introduces an ordering probelm
- when you create replicas before the service, pods dont get Env Vars
- scheduler might put both pods on the same machine
- it'll take entire service down if it dies

*** DNS
- DNS  addon service, that automatically assigns dns names to other services

#+BEGIN_SRC shellbang
$ nslookup my-service
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      my-service
Address 1: 10.98.77.126 my-service.default.svc.cluster.local
#+END_SRC

** securing the service
- before exposing the service to the internet, you want to make sure the communication channel is secured
- we need
  - self signed certificates for https
  - an nginx server configured to use the certificates
  - a secret that makes the certificates accessible to pods

** exposing the service
- some poarts of app may need to expose a service onto an exterman IP
- two ways of doing this
  - nodeport
  - loadbalancer
* [[https://kubernetes.io/docs/concepts/services-networking/ingress/][ingress]]
** overview
- manages external access to services, typically HTTP
- can provide load balancing, SSL termination, name-based virtual hosting
** terminology
*** node:
single virtual/physical machine in cluster
*** cluster
- a group of nodes firewalled from the internet
- primary compute resource managed by kubernetes
*** edge router
- router that enforces the firewall policy
- could be gateway managed by cloud provider or physical hardware
*** cluster network
- a set of links, logical or physical that facilitate communication within cluster
*** service
- identifies a set of pods using label selector
- services are assumed to have virtual IP only orutable within the cluster
  
** ingress
- exposes HTTP and HTTPS routes from outside the cluster to services within the clluster
- traffing routing is controlled by ingress

#+BEGIN_SRC
 internet
    |
 ingress
    |
  -------
[services]
#+END_SRC

- ingress can be configured to give services
  - externally-reachable URLs
  - loadbalance traffic
  - terminate SSL
  - offer name based virtual hosting
- doesn't expose arbitrary ports or protocol
- exposing services other than HTTP and HTTPS to internet typically uses a service of type nodeport/loadbalancer
** ingress controller
** ingress rules
- each http rule contains the following
  - optional host
    - if not defined, rule applies to all inbound HTTP traffic through the IP adress specified
    - if host is provided, rule applies to the host
  - list of paths
    - each path has an associataed backend servicename and serviceport
    - both the host and path must match the incoming request before loadbalancer direct traffic to the referenced service
  - backend
    - combination of service and port name
    - default backend is used if request don't match the path in the spec

#+BEGIN_SRC yaml
apiVersion:
kind:
metadata:
spec:
  backend:
  tls: []               #tls configs for all the hosts mentioned in the rule
  - hosts: []
  - secretName:
  rules:                # define all the rules
  - host:               #define the host
    http:
      paths:            #define all the paths for the given host
      - path:
        backend:        #define the servicename and port where traffice will be directed
          serviceName:
          servicePort:
#+END_SRC

** ingress types
*** single service ingress
- expose a single service
- can also be done using default backend with no rules

* [[https://kubernetes.io/docs/concepts/services-networking/network-policies/][network policies]]
- specificaiton about how groups of pods are allowed to communicate with each other and other network endpoints
- use labels to select pods and define reules which specify what traffic is allowd to the selected pods
- by default pods are non isolated, accept traffic from any source
- becomes isolated by having a network policy that selects them
- then the pod will reject nay connection that are not allowed by the policy
* [[https://kubernetes.io/docs/concepts/storage/volumes/][volumes]]
** overview
- files in container are ephemeral
- when container crashes, kubelet will restart it, container will start with a clean state, files will be lost
- running containers together in a pod often needs to share files between them
- kubernetes volumes solves these problems
** background
- kubernetes volumes has explicit lifetime, same as pod that encloses it
- volume outlives any container that run within the piod
- data is preserved across container restart
- kubernetes supports many type of volumes
- pod can use any number of them simultaneously
- volume is just a directory with some files in it, which is accessable from the containers in a pod
- how that directory comes to be, the medium that backs it, contenets of it are determined by the volume type
- to use a volume, pods specify =.spec.volume= and where to mount that volume into container =.spec.containers.volumeMounts=
- 
* [[https://kubernetes.io/docs/concepts/storage/persistent-volumes/][persistent volume]]
** overview
- persistentvolume provides an api that abatracts details of how storage is proided from how it is consumed
- two api resources
  - persistentVolume
  - persistentVolumeClaim
  
** persistentVolume
- storage that has been provisioned by an admin
- it is a resource in the cluster like a node is a cluster resource
- lifecycle independent of any individual pods

** persistentVolumeClaim
- request for storage by user
- similar to pods
- pods consume node resources
- pvc consume pv resource
- pods can request resources (cpu and memory)
- pvc can request specific size and access mode
** lifecycle of volume and claim
- pvs are resoruces in the cluster
- pvcs are requests for those resources
  
*** provisioning
**** static
- cluster admin creates number of pvs
- they carry the details of the real storage

**** dynamic
- none of the static pvs the admin created matches users =PersistentVolumeClaim=
- cluster may try to dynamically provision a volume specially for that pvc
- this is based on storageclass
- pvc may request storageclass
- admin must have created and configured that class
- to enable dynamic storage provisioning, cluster admin needs to enable =DefaultStorageClass=
** binding
- a user creates/(has already created for dynamic provisioning) a PersistentVolumeClaim with specific amount of storage requested and access modes
- control loop in the master watches the new pvcs, find matching pv and binds them together
- if pv was dynamically provisioned for new pvc, loop always bind that pv to the pvc
- otherwise user will always get at least what they asked for
- the volume may excess of what was requested
- once bound, PersistentVolumeClaim binds are exclusive
- pvc to pv binding is one-to-one mapping
- claims will remain unbound if matching volume doesn't exist
- claim will bound as matching volumes become available
- cluster provisioned with many 50Gi pvs would not match a pvc requesting 100Gi

** using
- pods use claims as volume
- cluster inspects the claim to find the bound volume
- then mounts the volume in the piod
- for volume supporting multiple access modes, user defines which mode is desired when using the claim
- once user has a claim and claim is bound, the bound pv belongs to the user as long as they need it

** storage object in use protection
- purpose of storage objecct in use protection is to ensure that pvc in active use by pod and pv that are bound to pvc are not removed from the system as they might result in data loss
- when is enabled, if user deletes a pvc in active use by pd, pvc is not removed immediately
- removal is postponded until the pvc is no longer actively used by any pods
- if admin deletes a pv that is bound to a pvc, the pv is not removed immediately
- pv removal postponded until pv is not bound to the pvc any more

** reclaiming
- when user is done with volume, they can delete the pvc
- this allows reclaiming of the resource

*** retain
- manual reclamation of the resource
- when pvc is deleted, pv still exists
- volume is considered released
- it is not yet available for another claim because previous claiman'ts data remains on the volume
- admin can manually reclaim the volume
  - delete the pv, assiciated storage asset in external infra still exists after pv is deleted
  - manually clean up data
  - manually delete associated storage asset

*** deleteion
- removes the pv object from kubernetes, as well as the associated storage asset in the external infra
- volumes that were dynamically provisioned inherit reclaim policy of the storageclass, defaults to delete

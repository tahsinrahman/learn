* overview
- we use kubernetes api objects to describe cluster's desired state
- what applications to run
- what container image they use
- no of raplicas
- network and disk/cpu usage

- kubernetes performs variety of tasks automatically
- starting/restarting containers
- scaling no of replicas

- kubernetes master- three processes that run on a single node
- kube-apiserver
- kube-controller-manager
- kube-scheduler

- other/worker node - two processes on each node
- kubelet: communicates with the master
- kube-proxy: proxy network that reflects networking services on each node

* kubernetes-objects
- basic objects
- pod
- service
- volume
- namespace

- high level abstractions: controllers
- based on basic objects
- provides additional functionality

- controllers
- replicaset
- deployment
- statefulset
- daemonset
- job

* kubernetes control plane
- govern how kubernetes communicates with the cluster
- maintains record of all objects
- runs continuous loop to manage those objects state
- responses to change in the claster: work to make actual state to match desired state for all objects


*** kubernetes master
- maintains desired state of the cluster

*** kubernetes nodes
- machines (vm/physical server)
- run apps
- managed my master

* [[https://kubernetes.io/docs/concepts/overview/components][kubernetes components]]
** master components
*** kube-apiserver
- exposes the kubernetes api
- front-end for kubernetes control plane

*** etcd
- key-value store: storeage for all cluster data

*** kube-scheduler
- watches newly created pods
- assigns nodes to them

*** kube-controller-manager
- runs controller(control loop: watches the state of the cluster, works to move current state to desired state)
- each controller: separate process, single binary, runs as signle process
- node controller: notice and responds when nodes go down
- replication controller: maintains the correct number of pods
- endpoints controller: populates the endpoints objects(joins services and pods)
- service account and token controller: create accounts and api access tokens for new namespace

*** cloud controller manager
- runs controller that interacts with underlying cloud providers
- controllers:
- node controllers: checks the cloud provider to determine if a node has been deleted
- router controllers: sets up routes in underlying cloud infra
- service controller: create/update/delete cloud provider node balancer
- volume controller: attach/mount/interacts with cloud provider volumes

** node components
- runs on every nodes
- maintains running pods

*** kubelet
- makes sure that containers are running in a pod
- pods are running and healthy
- doesn't manage containers

*** kube-proxy
*** container runtime
* [[https://kubernetes.io/docs/concepts/overview/kubernetes-api/][kubernetes api]]
* [[https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/][working with kubernetes objects]]
** overview
- persistent entities in the kubernetes system
- kubernetes uses these entities to represent state of the cluster
  - what apps are running, on which nodes
  - resources available to the apps
  - policies around how those apps behave(restart policies, upgrade, fault-tolerance)
- once objects are created, kubernetes works to ensure that objects remain exist

*** object spec and status
 - every object includes two nested object fields that govern the object's configuration: object spec and object status
 - object spec
   - one must provide
   - describes desired state for the object
 - object status
   - actual state of the object
   - supplied and updated by kubernetes
   - kubernetes control plane manages objects actual state to match desired state
** names
** namespaces
- kubernetes supports multiple vertical clusters backed by same physical server
- these virtual clusters are namespaces

*** when to use multiple namespaces
- many users spread across multiple teams/projects
- divide cluster resources between multiple users

*** viewing namespaces
#+BEGIN_SRC shebang
$ kubectl get namespaces
NAME          STATUS    AGE
default       Active    1d
kube-system   Active    1d
kube-public   Active    1d
#+END_SRC

- three initial namespaces
  - default: default namespaces
  - kube-system: namespace for objects created by kubernetes system
  - kube-public: readable by all users
  - 
** labels and selectors
*** labels
- key-value pairs that are attached to an object
- identify attributes of an object that are meaningful to users
- organize and select subset of objects
- labels can be attached on creation/modified later
- 

*** selectors
- many objects  have same lables
- user can identify a set of objects via label selector
- empty label selector selects every object
- null label selector selects no object
- two types of selectors: equality based, set based
  - equality based: filter by keys and values
#+BEGIN_SRC yaml
apiVersion: v1
kind: pod
metadata:
  name: cuda-test
spec:
  containers:
  - name: cuda-test
    image: "k8s.gcr.io/cuda-vector-add:v0.1"
    resources:
      limits:
        nvidia.com/gpu: 1
  nodeSelector:
    accelerator: nvidia-tesla-p100
#+END_SRC
  - set based: allows filtering keys according to a set of values
#+BEGIN_SRC
environment in (production, qa)
tier notin (frontend, backend)
partion
!partition
#+END_SRC

*** watch and list filtering
#+BEGIN_SRC shellbang
kubectl get pods -l environment=production,tier=frontend
kubectl get pods -l 'environment in (production), tier in(frontend)'
kubectl get pods -l 'environment in (production, qa)'
kubectl get pods -l 'environment,environment notin (frontend)'
#+END_SRC

#+BEGIN_SRC yaml
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}
#+END_SRC
** annotations
- non-identifying metadata
- one can use either label or annotation
- labels are used to select and find collection of objects that satisfy certain conditions
- annotations are not used to identify and select objects
- build, release, image information like timestamp, git branch, pr number, hash

** field selector
- select kubernetes objects based on value of one or more resource fields
$ kubectl get pods --field-sellector status.phase=Running
* [[https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/][kubernetes object management]]
* kubernetes architecture 
** nodes
- worker machines
- vm/physical machine
- managed by master
- runs containers
- includes
  - container runtime
  - kubelet
  - kubeproxy
  

*** node controller
- master component
- manages various aspects of nodes
- assigns CIDR block
- update list of nodes
- monitor nodes' health
- when node becomes unhealthy, it asks the cloud provider if it is still available, if not deletes it from list of nodes

** master-node communication
*** cluster to master
- communication path terminate at apiserver(other master components are not exposed)
- apiserver listens at HTTPS 443 port
- nodes contain public root certificate of the cluster
- when pods are created, 

*** master to cluster
**** apiserver to kubelet
used for:
- fetching logs for pods
- attaching to running pods
- providing kubelet's port-forwarding functionality
- 

**** apiserver to nodes/pods/services
- HTTP connection
- not authenticated/encrypted
- not confurrently safe
** cloud controller manager
- design based on plugin mechanism
- allows cloud providers integrate with kubernetes using plugins
- 

*** without cloud controller manager
#+DOWNLOADED: /tmp/screenshot.png @ 2018-11-30 12:53:16
[[file:kubernetes%20architecture/screenshot_2018-11-30_12-53-16.png]]

*** with cloud controller manager
  
#+DOWNLOADED: /tmp/screenshot.png @ 2018-11-30 12:47:37
[[file:kubernetes%20architecture/screenshot_2018-11-30_12-47-37.png]]



*** cloud controller manager runs:
- node controller
- router controller
- service controller
- persistent volume label controller
**** node controller
- initialize a node with cloud specific zone/region label
- initialize a node with cloud specifi detail, type and size
- obtain node's network adress and hotname
- check if node is deleted from cloud, if yes then delete the kubernetes node object
**** route  controllers
- configures routes so containers on different nodes in the cluster can communicate with each other
- only applicable for GCE clusters
**** service controller
- listens to service create, update, delete events
- configures cloud load balancers
- 

**** persistent volume labels controller
- applies labels on volumes when crated
- removes the need of users manually set the labels
- these labels are essential for scheduling pods, as they only work within the region/zone that they are in, so pods need to be in same region
- 

** kubelet
- node controller contains cloud dependent functionality of the kubelet
- before, kubelet was responsible for cloud-specific details(ip address, region/zone labels, instant type information)
- now, the initialization operations are done in ccm
- now, kubelet initialize a node without cloud specific information
- new nodes are unschedulable until ccm initializes the node with cloud specific informations
* [[https://kubernetes.io/docs/concepts/containers/][containers]]
* [[https://kubernetes.io/docs/concepts/workloads/pods/][pods]]
** overview
*** understanding pods
**** overview
     - smallest deployable object
     - basic building block
     - smallest and simplest unit
     - represents a running process
     - pods encapsulate
       - application containers(or multiple containers)
       - storage
       - unique ip
       - options that govern how the containner(s) should run
     - containers are tightly coupled and share resource
     - single instance of an application
       - if scaled horizontally, multiple pods, one for each instance
       - replicated pods created and managed by controller

**** pods that run single containers
     - most common use case
     - pod as a wrapper around single container
     - kubernetes manages pod rather than conntainers directly

**** pods that run multiple containers
     - encapsulate an application that composed of multiple co-located containers that are tightly coupled and need to share resource
     - form a single cohesive unit of service

**** how pods manage multiple containers
     - multiple cooperating processes as containers form a cohesive unit of service
     - containers are automatically co-located and co-scheduled on same node
     - share resource, dependencies
     - communicate with each other
     - coordinate when and how to terminate
     - advanced use case
     - only use when containers are tightly coupled

**** shared networking
     - each pod is assigned a unique ip address
     - every container in a pod shares the network namespace including ip addresses and ports
     - containers can communicate with each other using *localhost*

**** shared storage
     - pod specify a set of shared storage volumes
     - all containers can access the shared volumes
     - allows persistent data in a pod
*** working with pods
    - you'll rarely create individual pods directly
      - pods are designed as relatively short lived, disposable entities
      - when created, it is scheduled to run on a node
      - remains on that node until process terminates, then pod is deleted
      - do not self-heal
      - pods get deleted
        - if node fails
        - scheduling operations fails
        - lack of resource
        - node maintainance
    - controller handles the work managing the disposable pod instances
    - so, it is far more common to manage pods using a controller
*** pods and controllers
    - create and manage multiple pods
    - handles replication, rollout and self healing capabilities
    - if node fails, controller might automatically replace the pod by scheduling an identical replacement on different node
    - example of controllers that contain one or more pors
      - deployment
      - statefulset
      - daemonset
*** pod template
    - pod specifications which are included in other objects such as replication controller, jobs, daemonset
    - uses pod templates to make actual pods

    #+BEGIN_SRC yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: myapp-container
      image: busybox
      command: ['sh', '-c', 'echo hello kubernetes! && sleep 3600']
    #+END_SRC

    - after creation, pods can't be updated
    - new template has no direct effect on pods already created
    - but pods created by controllers can be updated directly
** pods
   - pods are isolated by linux namespaces, cgroups
   - within pod, individual containers are furher sub-isolationed
   - containers within a pod share an ip addresses and port space
   - find each other via localhost
   - communicate with each other using standard inter process communications like semaphores or shared memory
   - containers in different pods have different ip addresses
   - can't communicate by IPC
   - they communicate via pod ip address
   - 

** termination of pods
   - pods are gracefully terminated
   - when users request deleteion of pod, system records the intended grace period and TERM signal is sent to main process of each container
   - once grace period has expired, the KILL signal is send to those processes
   - pods are then deleted from API server
** container probes
- probe is a diagnostic performed periodically by kubelet
- kubelet calls a handler implemented by the container
- 3 types of handlers
  - runs specific command inside the container. diagnostic is successful if command exists with status code 0
  - performs TCP check against container's IP on spefcified port. diagnostic successful if port is open
  - performs HTTP GET request against container's IP address on specified port and path. Diagnostic successful if response code betwen [200, 400)
- optionally performs two types of probles
  - livenessProbe:
    - indicates whether the container is running
    - if fails, kubelet kills the container
    - container is then subjected to its restart policy
    - if container does not provide a liveness probe, default state is success
  - readinessProbe:
    - indicates whether the container is ready to service requests
    - if fails, the endpoint controller removes pod's ip address from all services that match the pod
    - if container does not provide a readinessProbe, default state is success
    
*** when to use probes
- if container crashes on its own whenever it encounters an issue or becomes unhealty, no need for liveness probe
- if you want to send traffic to pod when a probe succeds, specify readiness probe
- the pod will start without recieving any traffic, only start recieiving after probe starts succeeding
- if container needs to load large data, configuring files, specify readiness probe
** init containers
** pod presets
*** overview
- inject additional runtime requirements into a pod at creation time
- use label selectors to specify the pods to which the preset applies
- allows not to have explicitly provide all information for every pod
- authors of pod templates consuming a specific service do not need to konow all the details of that service

*** how it works
- admission controller(PodPreset) which when enabled, applies pod presets to incoming pod creation request
- when pod creation request occures, system does
  - retrieve all PodPrestets
  - checks if label selectors of any PodPreset matches the labels of the pod
  - attempts to merge various resources defined by podpreset
  - on error, through event, and create the pod without injecting any resource from podpreset
  - annotate the the resulting modified pod spec to indicate that it has been modified by podpreset
- when podpreset applies to one or more pods
  - modifies the pod specs
  - for Env EnvFrom VolumeMounts, modifies the container spec for all containers
  - for Volume, modifies the pod spec
** discuptons

* [[https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/][replica set]]
- next generation replication controller
- only difference between replicaset and replication controller is selector support
- replicaset supppports the new set based selector
- replication controller only supports equality based selector


** how to use replicaset
- most kubectl commands that supports replication controller also supports replicasets
- only exception is rolling-update command
- for rolling update functionality, one should use deployments instead
- replicasets can be used independently
- mainly used by deployments


** when to use replicaset
- replicaset ensures that spcified number of pod replicas are running at any given time
- we should use deployments instead of directly using replicasets
* [[https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/][replicationcontroller]]
- ensures that specified number of pod replicas are running at any one time
- makes sure that pods are always up and available

** how replicationcontroller works
- if too many pods, terminates extra pods
- if too few pods, creates more pods
- pods managed by replicationcontroller are automatically replaced if they fail, deleted or terminated
* [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/][deployments]]
- controller that provides declarative udpates for pods and replicasets
- you describe a desired state in deployment, and deployment controller changes tha actual state to the desired 
* [[https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/][statefulset]]
- used to manage stateful applications
- manages the deployment and scaling of a set of pods
- provides gurantee about ordering and uniquness of these pods
- like deployment, manages pods that are based on an identical container spec
- unlike deployments, maintains a sticky identity for each of their pods
- these pods are of same specs, but are not interchangeable
- each has a persistent identifier
- that is maintained across any rescheduling


** using statefulsets
- stable unique network identifier
- stable, persistent storage
- ordered, graceful deployment and scaling
- ordered, automated rolling updates
- if application doesn't require any stable identifiers or ordered deployment, deletion or scaling, you should deploy it with deployments

** limitaions
- deleting/scaling down doesn't delete the volumes associated with the stateful
- it is because of data safety
- do not guarantee on the termination of pods when statefulset is deleted
- to achive ordered and graceful termination of the pods, it is possible ot scale down to 0 prior to deletion
* [[https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/][daemonset]]
- ensures that all of some nodes run a copy of a pod
- as nodes are added, pods are added to them
- as nodes are deleted, pods are deleted
** use cases
- running cluster storage daemon
- running log collection daemon
- running node monitoring daemon on every node
* [[https://kubernetes.io/docs/concepts/services-networking/service/][services]]
** overview
- pods are mortal
- pods get own ip, but they can't be relied upon to be stable over time
- if some st of pods(backend) provides functionality to other pods(frontend), how do those frontends find out and keep track of which backends are in that set
- service: defines a logical set of pods and a policy by which to access them
- the set of pods targeted by a service is determined by a label selector
- image processing backend with three replicas
- frontend do not care which backend they use
- backend pods can change, but frontend clients should not be aware of them
- for kubernetes native apps, kubernetws offers endpoints api that is updated whenever the set of pod in a service change
- for non native apps, kubernets offers virtual ip based bridge to services which redirects to the backend pods

** defining a service
#+BEGIN_SRC yaml
kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
#+END_SRC
- it'll create a service named ~my-service~
- it targets 9376 on any pod with ~app=myapp~ label
- service will have an IP (clusterIP)
- maps incoming port to any targetPort
- by default, targetPort is same as port
- targetPort can be string, the name of a port in the pods
- so you can change the port number that pod exposes without breaking service

- kube-proxy watches the kubernets master for addition and removal of service
- for each service, it opens a port(randomly chosen) on local node
- any connecton to this(proxy) port will be proxied to the one of service's backend pods
- it installs iptable rules which caputre traffic to the service's clsuterIP and port
- then redicects that traffic to the proxy port which proxies the backend pods
** service without selectors
- service abstracts access to pods
- they can also abstract other bacnekds
  - you want to point your service to a service in another namespace
  - you are migrating your workload to kubernetes and some of your backends run outside of kubernetess
  - you want to have external database cluster in production, but in test you use your own database

#+BEGIN_SRC yaml
kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
#+END_SRC
- service has no selector
- corresponding endpoints object will not be created
- you can manually map the service to specific endpoints

** headless services
- sometimes you don't need/want loadbalanceing and single service IP
- headless service in that case
- "none" in clusterIP field
- cluster IP not allocated
- kube-proxy doesn't handle these services
- no loadbalancing or proxying done by the platform for them

*** with selectors
- headless services with selectors, endpoints controller creates endpoints records
- modifies the DNS configureation to recotd addresses that point directoly to the pods

*** without selectors
- endpoint controller don't create endponts records

** publishing services:  service types
- ClusterIP
- NodePort
- LoadBalancer
- ExternalName

*** ClusterIP
- exposes service on cluster-internal IP
- service only reachable from within the cluster
- default service type

*** NodePort
- exposes service on each node's ip at a specific port
- a clusterIP service, to which the nodeport service will route, is automatically created
- you can contact nodeport service from outside of the cluster, <NODE_IP>:<NODE_PORT>

*** LoadBalancer
- exposes service externally using cloud provider's load balancer
- nodeport and clusterIP service to which external load balancer will route are automatically created
- for cloud providers that support external load balancers, it'll provision a load balancer for your service
- traffic from external load balancer will be directed at the backend pods

*** ExternalName
- maps the service to the contents of the externalName field
- no proxying of any kind is set up
- maps a service to a DNS name

#+BEGIN_SRC yaml
kind: Service
apiVersion: v1
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
#+END_SRC

- when looking up the host =my-service.prod.svc.cluster.local= dns service will return =my.database.example.com=
- redirection happens in DNS label rather than proxying or forwarding

* [[https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/][DNS for services and pods]]
** overview
- kubernets schedules DNS pod and service on the cluster
- configures the kubelet to tell containers to use the DNS service's IP to resolve DNS names
- every service is assigned a DNS name
- service name =foo= in namespace =bar=
- pod running in namespace =bar= can look up this service by =foo=
- pod running in namespace =xyz= can look up by =foo.bar=

** services
*** A record
- normal (not headless) services are assigned a DNS A record of the form =my-svc.my-namespace.svc.cluster.local=
- this resolves to the clusterIP of the service
- headless services are also assigned DNS A record of the form =my-svc.my-namespace.svc.cluster.local=
- this resolves to the set of IPs of the pods selected by the service

*** SRV record
- SRV records are created for named ports that are part of normal/headless service
- for named port, =_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local=
- this resolves to =my-svc.my-namespace.svc.cluster.local=
- for headless service, this resolves to a set of IPs of the pod
- contains the port number and the domain name of the pod =auto-generated-name.my-svc.my-namespace.svc.cluster.local=

** pods
*** A record
- pods are assigned DNS A record =pod-ip-address.my-namespace.pod.cluster.local=
- pod ip =1.2.3.4= in namespace =default=, dns =1-2-3-4.default.pod.cluster.local=

*** pod's hostname and subdomain fields
- when pods are created, hostname is =metadata.name=
- optional =hostname= field
- when specified, takes precedence over pod's name to be the hostname
- also have =subdomain= field
- pos with =nostname:foo= and =subdomain:bar= in namespace =my-namespace=, DNS =foo.bar.my-namespace.svc.cluser.local=

* [[https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/][connecting applications with services]]
** overview
- docker use host private networking
- containers can talk to each other only if they are on the same machine
- to commincate across nodes, there must be allocated ports on the machine's own ip address
- they are then forwarded or proxied to the containers
- so, containers must co-ordinate which ports they use, or ports must be allocated dynamically
- coordinating ports is very difficult to do at scale
- exposes users to cluster-level issues outside of their control
- kubernetes assumes that pods can communate with other pods, regardless of which host they land on
- every pod is given own cluster-private-ip
- no need to explicitly create links between pods or map host ports to container ports
- containers within a pod, can reach each other's ports on localhost
- all opds in a cluster can see each other wihtout NAT

** exposing pods to cluster
#+BEGIN_SRC yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx
spec:
  selector:
    matchLabels:
      run: my-nginx
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerPort: 80
#+END_SRC

#+BEGIN_SRC shellbang
$ kubectl get pods -l run=my-nginx -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP            NODE
my-nginx-77f56b88c8-cj2fr   1/1     Running   0          14s   172.17.0.20   node-1
my-nginx-77f56b88c8-ptt8t   1/1     Running   0          14s   172.17.0.19   node-2
#+END_SRC

- can ssh into any node and curl both IPs
- kubectl exec into any pod and curl into both IPs

** creating service
- when node dies, pod dies with it, deployment creates new pods with different IPs
- this is the problem that servicce solves
- service defines a logical set of pods that all provide same functionality
- when created, each service is assigned a unique IP address(clusterIP)
- this is tied to the lifespan of the service, will not change while service is alive
- pods can be configured to talk to the service
- communication to the service will be automatically load-balanced out to some opd that is a member of the service

#+BEGIN_SRC yaml
apiVersion: v1
kind: Service
metadata:
  name: my-nginx
  labels:
    run: my-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    containerPort: 80
  selector:
    run: my-nginx
#+END_SRC

- it'll create a service which targets TCP port 80 on any pod with the  ~run: my-nginx~ label
- targetPort: the port the container accepts traffic on
- port: the service port, which can be any port that other pods use to access the service
- you can curl the nginx service on ~<CLUSTER_IP>:<PORT>~ from any node/pod


- a service is backed by a group of pods
- these pods are exposed through endpoints
- when pod dies, it is automaticall yremoved from the endpoints
- new pods matching service's selector will be automatically added to endpoints

#+BEGIN_SRC shellbang
$ kubectl get pods -l run=my-nginx -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP            NODE
my-nginx-77f56b88c8-cqqsg   1/1     Running   0          1h    172.17.0.19   minikube
my-nginx-77f56b88c8-v22kz   1/1     Running   0          1h    172.17.0.20   minikube

$ kubectl get endpoints my-service
NAME         ENDPOINTS                       AGE
my-service   172.17.0.19:80,172.17.0.20:80   1h
#+END_SRC

** accessing service
two primary modes for finding service
- environment variable (works out of the box)
- DNS (requires CoreDNS addon)


*** environment variable
- when pod runs on a node, kubelet adds a set of env vars for each active service
- this introduces an ordering probelm
- when you create replicas before the service, pods dont get Env Vars
- scheduler might put both pods on the same machine
- it'll take entire service down if it dies

*** DNS
- DNS  addon service, that automatically assigns dns names to other services

#+BEGIN_SRC shellbang
$ nslookup my-service
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      my-service
Address 1: 10.98.77.126 my-service.default.svc.cluster.local
#+END_SRC

** securing the service
- before exposing the service to the internet, you want to make sure the communication channel is secured
- we need
  - self signed certificates for https
  - an nginx server configured to use the certificates
  - a secret that makes the certificates accessible to pods

** exposing the service
- some poarts of app may need to expose a service onto an exterman IP
- two ways of doing this
  - nodeport
  - loadbalancer
* [[https://kubernetes.io/docs/concepts/services-networking/ingress/][ingress]]
** overview
- manages external access to services, typically HTTP
- can provide load balancing, SSL termination, name-based virtual hosting
** terminology
*** node:
single virtual/physical machine in cluster
*** cluster
- a group of nodes firewalled from the internet
- primary compute resource managed by kubernetes
*** edge router
- router that enforces the firewall policy
- could be gateway managed by cloud provider or physical hardware
*** cluster network
- a set of links, logical or physical that facilitate communication within cluster
*** service
- identifies a set of pods using label selector
- services are assumed to have virtual IP only orutable within the cluster
  
** ingress
- exposes HTTP and HTTPS routes from outside the cluster to services within the clluster
- traffing routing is controlled by ingress

#+BEGIN_SRC
 internet
    |
 ingress
    |
  -------
[services]
#+END_SRC

- ingress can be configured to give services
  - externally-reachable URLs
  - loadbalance traffic
  - terminate SSL
  - offer name based virtual hosting
- doesn't expose arbitrary ports or protocol
- exposing services other than HTTP and HTTPS to internet typically uses a service of type nodeport/loadbalancer
** ingress controller
** ingress rules
- each http rule contains the following
  - optional host
    - if not defined, rule applies to all inbound HTTP traffic through the IP adress specified
    - if host is provided, rule applies to the host
  - list of paths
    - each path has an associataed backend servicename and serviceport
    - both the host and path must match the incoming request before loadbalancer direct traffic to the referenced service
  - backend
    - combination of service and port name
    - default backend is used if request don't match the path in the spec

#+BEGIN_SRC yaml
apiVersion:
kind:
metadata:
spec:
  backend:
  tls: []               #tls configs for all the hosts mentioned in the rule
  - hosts: []
  - secretName:
  rules:                # define all the rules
  - host:               #define the host
    http:
      paths:            #define all the paths for the given host
      - path:
        backend:        #define the servicename and port where traffice will be directed
          serviceName:
          servicePort:
#+END_SRC

** ingress types
*** single service ingress
- expose a single service
- can also be done using default backend with no rules

* [[https://kubernetes.io/docs/concepts/services-networking/network-policies/][network policies]]
- specificaiton about how groups of pods are allowed to communicate with each other and other network endpoints
- use labels to select pods and define reules which specify what traffic is allowd to the selected pods
- by default pods are non isolated, accept traffic from any source
- becomes isolated by having a network policy that selects them
- then the pod will reject nay connection that are not allowed by the policy
* [[https://kubernetes.io/docs/concepts/storage/volumes/][volumes]]
** overview
- files in container are ephemeral
- when container crashes, kubelet will restart it, container will start with a clean state, files will be lost
- running containers together in a pod often needs to share files between them
- kubernetes volumes solves these problems
** background
- kubernetes volumes has explicit lifetime, same as pod that encloses it
- volume outlives any container that run within the piod
- data is preserved across container restart
- kubernetes supports many type of volumes
- pod can use any number of them simultaneously
- volume is just a directory with some files in it, which is accessable from the containers in a pod
- how that directory comes to be, the medium that backs it, contenets of it are determined by the volume type
- to use a volume, pods specify =.spec.volume= and where to mount that volume into container =.spec.containers.volumeMounts=
- 
* [[https://kubernetes.io/docs/concepts/storage/persistent-volumes/][persistent volume]]
** overview
- persistentvolume provides an api that abatracts details of how storage is proided from how it is consumed
- two api resources
  - persistentVolume
  - persistentVolumeClaim
  
** persistentVolume
- storage that has been provisioned by an admin
- it is a resource in the cluster like a node is a cluster resource
- lifecycle independent of any individual pods

** persistentVolumeClaim
- request for storage by user
- similar to pods
- pods consume node resources
- pvc consume pv resource
- pods can request resources (cpu and memory)
- pvc can request specific size and access mode
** lifecycle of volume and claim
- pvs are resoruces in the cluster
- pvcs are requests for those resources
  
*** provisioning
**** static
- cluster admin creates number of pvs
- they carry the details of the real storage

**** dynamic
- none of the static pvs the admin created matches users =PersistentVolumeClaim=
- cluster may try to dynamically provision a volume specially for that pvc
- this is based on storageclass
- pvc may request storageclass
- admin must have created and configured that class
- to enable dynamic storage provisioning, cluster admin needs to enable =DefaultStorageClass=
** binding
- a user creates/(has already created for dynamic provisioning) a PersistentVolumeClaim with specific amount of storage requested and access modes
- control loop in the master watches the new pvcs, find matching pv and binds them together
- if pv was dynamically provisioned for new pvc, loop always bind that pv to the pvc
- otherwise user will always get at least what they asked for
- the volume may excess of what was requested
- once bound, PersistentVolumeClaim binds are exclusive
- pvc to pv binding is one-to-one mapping
- claims will remain unbound if matching volume doesn't exist
- claim will bound as matching volumes become available
- cluster provisioned with many 50Gi pvs would not match a pvc requesting 100Gi

** using
- pods use claims as volume
- cluster inspects the claim to find the bound volume
- then mounts the volume in the piod
- for volume supporting multiple access modes, user defines which mode is desired when using the claim
- once user has a claim and claim is bound, the bound pv belongs to the user as long as they need it

** storage object in use protection
- purpose of storage objecct in use protection is to ensure that pvc in active use by pod and pv that are bound to pvc are not removed from the system as they might result in data loss
- when is enabled, if user deletes a pvc in active use by pd, pvc is not removed immediately
- removal is postponded until the pvc is no longer actively used by any pods
- if admin deletes a pv that is bound to a pvc, the pv is not removed immediately
- pv removal postponded until pv is not bound to the pvc any more

** reclaiming
- when user is done with volume, they can delete the pvc
- this allows reclaiming of the resource

*** retain
- manual reclamation of the resource
- when pvc is deleted, pv still exists
- volume is considered released
- it is not yet available for another claim because previous claiman'ts data remains on the volume
- admin can manually reclaim the volume
  - delete the pv, assiciated storage asset in external infra still exists after pv is deleted
  - manually clean up data
  - manually delete associated storage asset

*** deleteion
- removes the pv object from kubernetes, as well as the associated storage asset in the external infra
- volumes that were dynamically provisioned inherit reclaim policy of the storageclass, defaults to delete

** wriging portable configuration
- do include pvc object in bundle of config, alongside deployment, configmaps etc
- do not include pv object in the config, as user may not have permissions to create pv
* [[https://kubernetes.io/docs/concepts/configuration/overview/][configuration best practices]]
** general tips
- when defining configs, specify latest stable API version
- config files should be stored in version control before pushed to cluster
- this helps to quickly roll back
- also allows cluster re-creation and restoration
- write configs in YAML instead of JSON
- group related objects into single file whenever it makes sense
- don't specify default values unnecessary, simple minimal configuration
- put object descriptions in annotations

** naked pods vs replicasets deployments and jobs
- don't use naked pods
- naked poids will not be reschedules in the event of a node failure
- deployment which both creates replicaset to ensure desired no of pods are always available, specifies a strategy to replace pods
- is almost always preferable to create pods directly
- except =restartPolicy: Never= scenarios
- job is also appropriate in this case

** services
- create service before its corresponding backend workloads(deployemnt/replicasets)
- create service before any workloads that need to access it
- when containers is created, it provides environment variables pointing to all services that were running when the container is started
- if service =foo= exists, all containers wil get
#+BEGIN_SRC bash
FOO_SERVICE_HOST=<service host>
FOO_SERVICE_PORT=<service port>
#+END_SRC
- don't use these env vars
- use dns name of the service instead
- don't specify =hostPort= to a pod unless absolutely necessary
- when you bind pod to =hostPort=, it limits the no of places the pod can be schedules
- if you need to acces to the port for debugging, you can use apiservice proxy or =kubectl port-forward=
- if you explicitly need to expose pod's port on the node, consider using =NodePort= service
- avoid using =hostNetwork=
- use =headless services= for easy service discovery when you don't need load balancing

** using labels
- define labels that identify semantic attributes of your appp or deployment
- =app: myapp, tier: frontemnd, phase: test, deployment: v3=
- a service can be made to span multiple deployments by omitting release specific labels from selector
- makes it easy to update a running service without downtime
- manipulate labels for debugging
- controllers and services match to the pods using selector labels
- removing label from a pod will stop it from being considered by a controller/from being served traffic by a service
- if remove existing label, controller wil lcreate new pod to take its place
- useful way to debug previouisly 'live' pod

** container image
- =imagePullPolicy: IfNotPresent= the image is pulled only if not already presetn locallay
- =imagePullPolicy: Always= the image is pulled every time the pod is started
- omitted if image tag is =:latest= or
- =imagePullPolicy= omitted if image tag is presetn, but not =latest= and =IfNotPresent= applied
- =imagePullPolicy: Never= image is assumed to exist locally. not attempt is made to pull the image
- should never use =latest= tag

** using kubectl
- use =kubectl apply -f <dirname>= or =kubectl create -f <directory>=
- using label selectors for =get= and =delete=
- use =kubectl run= or =kubectl expose= to quickly create single container deployment and services

* [[https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/][managing compute resources for containers]]
- you can optinally sepcify how much cpu and meory each container needs
- when specifyed, the scheduler can make better decisions about which node to place pods on

** resource types
- =spec.containres[].resources.limits.cpu=
- =spec.containres[].resources.limits.memory=
- =spec.containres[].resources.requests.cpu=
- =spec.containres[].resources.requests.memory=

** meaning of cpu
- a vcpu
- one cure
- one vcore
- one hyperthread
- fractional requests are allowed
- =spec.contaiers[].resources.requests.cpu: 0.5= guranteed half cpu that ask for 1 cpu
- =0.1= means =100m=, =one hundread milicpu=, =one hundread milicore=
- =100m= is preferred than =0.1m=, because of precisions
- absolute quantity
- =0.1= is same on single-core, dual-core, 48-core machine

** meaning of memory
- memory is measured in bytes
- plain integer or fixed point integer using =E P T G M K= or =Ei Pi Ti Gi Mi Ki= suffixes

** how pods with resource requests are scheduled
- when create pod, kube scheduler selects node for the pod
- each node has maximum capacity for each resource types
- scheduler ensures that sum of resource requests is less than capacity of the node
- protects agains resource shortage on a node

** how pods with resource limits are run
- passes the CPU and memory limits to docker
- =spec.containers[].resources.requests.cpu= is converted to core value, can be fractional, and multiplied by 1024
- used as the value of =--cpu-shares= flag of =docker run= command
- =spec.containers[].resources.limits.cpu= is converted to milicore value, multiplied by 100
- resulting value is total amount of cpu time container can use in 100ms time
- =spec.containers[].limits.memory= converted to integer, used as =--memory= flag for =docker run= command
- if container exceeds memory limit, might be terminated
- if restartable, kubelet will restart it
- if container exceeds memory request, it is likely that pod will be evicted whenever the node runs out of memory
  
** troubleshooting
- if scheduler cna't find any node where a pod can fit, pod remains unscheduled until a place can be cound
#+BEGIN_SRC bash
$ kubectl describe pod frontend | grep -A 3 Events
Events:
  FirstSeen LastSeen   Count  From          Subobject   PathReason      Message
  36s   5s     6      {scheduler }              FailedScheduling  Failed for reason PodExceedsFreeCPU and possibly others
#+END_SRC
- pod named 'frontend' failed to be scheduled due to insufficient cpu on he node
- similar error due to insufficient memory
- solved by, add more nodes
- termintaing unneeded pods
- check that pod is not larger than all the nodes
** local ephemeral storage
- kubelet's root directory (/var/lib/kubelet/) and log directory (/var/log/) are stored on root partion of the node
- this partition is also shared and consumed by pods via emptyDir volumes, container logs, image layers, container writable layers
- this partition is ephemeral
- applications cannot expect any performance SLA from this partition
- local ephemeral storage management only applies for root partition
- each container can specify =spec.containers[].resources.limits.ephemeral-storage=, =spec.containers[].requests.ephemeral-storage=
 
*** how pods with ephemeral-storage requests are scheduled
- each node ahs a max amount of local ephemeral storage it can provide for pods
- scheduler ensures that sum of resource requests is less than capacity

*** how pods with ephemeral storage limits run
- if containers writable layer and logs useage exceeds storage limits, pod will be eviicted
- for pods, if sum of local ephemeral storage from all containers and pods emptyDir volumes exceeds the limit, pod will be evicted

** extended resources
* [[https://kubernetes.io/docs/concepts/configuration/assign-pod-node/][assigning pods to nodes]]
- you can constrain a pod to be only able to run on particular nodes/prefer to run on particular nodes
- several ways, all use label selectors

** node selector
- simplest  form
- field of podspec
- specifies a map of key-value pairs
- for pods to be eligible to run on node, node must have each of the labels

*** attach label to node
#+BEGIN_SRC bash
# get list of nodes
$ kubectl get nodes
# add label
$ kubectl label nodes <node-name> <label-key>=<label-value>=
# verify
$ kubectl get nodes --show-labels
#+END_SRC

*** add nodeselector field to pod configuration
#+BEGIN_SRC yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    diskType: ssd
#+END_SRC

=$ kubectl get pods -o wide= to verify

** built-in node labels
- nodes come pre-populated with a standard set of lables
#+BEGIN_SRC 
kubernetes.io/hostname
failure-domain.beta.kubernetes.io/zone
failure-domain.beta.kubernetes.io/region
beta.kubernetes.io/instance-type
beta.kubernetes.io/os
beta.kubernetes.io/arch
#+END_SRC

- cloud specific
- not guranteed to be reliable

** node isolation/restriction
- adding labels allows targeting pods to specific nodes
- this can be used to ensure specific pods only run on nodes with certain isolation, security or regulatory proparties
- when using labels for this purpose, choosing label keys that can't be modified by kubelet on the node is strongly recommended
- =NodeRestriction= admission plugin prevents kubelets from setting/modifying labels with =node-restriction.kubernetes.io/= prefix
- ensure you are using Node Authorizer and enabled NodeRestriction Admission plugin
- add labels under =node-restriction.kubernetes.io/= prefix

** affinity and anti-affinity
- nodeSelector provides simple way to constrain pods to nodes with particular labels
- affinity, anti-affinity greately expands the types of constraints
- language is more expressive
- you can indicate the rul is "soft"/"preference" rather than hard requirement
- you can constrain against labels on other pods running on the node, rather than labels on node itself
- allows rules abour which pods can and can't be colocated
- two types of affinity, "node affinity" and "inter-pod affinity/anti-affinity"
- nodeselector wil be deprecated
- node affinity can express everything that nodeSelector can express

*** node affinity
- two types of node affinity
  - =requiredDuringSchedulingIgnoredDuringExecution=
  - =preferredDuringSchedulingIgnoredDuringExecution=
- hard and soft respectively
- former specifies rules that must be met for a pod to be scheduled onto a pod
- latter specifies pereference that the scheduler will try to enforce but will not guranttee
- IgnoredDuringExcution means, if labels on node change on runtijme such that affinity rules on a pod are no longer met, the pod will still continue to run on the node
- if specify both =nodeSelector= and =nodeAffinity= both must be satisfied to be scheduled
- if specify multiple =nodeSelectorTerms= associated with =nodeAffinity=, podd can be schedules if one of the =nodeSelectorTerms= is satisfied
- if specify multiple =matchExpressions= associated with =nodeSelectorTerms=, pod can be scheduled if all of =matchExpressions= is satiscied
- if remove/change the label node after pod is scheduled, pods won't be removed
- affinity selecton works only at the tiem of scheduling the pod
- =weight= in =preferredDuringSchedulingIgnoredDuringExecution= is in range 1-100
- for all node that meets the scheduling requirements, scheduler will compute a sum of weights
- this score is then combined with scores of other priority function of the node
- node with highest total scoore are most preferred
** pod affinity and anti-affinity
- allows to constrain which nodes your pd is eligible to be scheduled based on pables on pods that are already running on the node
- this pod should run(should not run for anti) in X node if X is already running one or more pods that meet rule Y
- Y is expressed as LabelSelector with associated list of namespaces
- pods are namespaced, label selector over pod labels must specify which namespaces the selector should apply to
- requires substantial amount of processing
- can slow down scheduling in large clusters
- not recommended for clusters larger than several hundred nodes

#+BEGIN_SRC yaml
affinity:
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: security
          operator: In
          values:
          - S1
      topologyKey: failure-domain.beta.kubernetes.io/zone
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S2
        topologyKey: kubernetes.io/hostname
#+END_SRC
- the pod can be schedules onto a node only if that node is in the same zone as at least one running pod that has label security:S1
- the pod is not prefered to be scheduled onto a node if it is already running a pod with label security:S2
- if topopogyKey:zone, the pod can't be scheduled onto a node if that node is in the same zone as a pod with label security:S2

* [[https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/][taints and tolerations]]
- node affinity, property of pods that attracts them to a set of nodes
- taints are opposite, allows a node to repel a set of pods
- taints are dolerations work together to ensure pods are not scheduled onto inappropriate nodes
- one or more taints are applied to a node
- this marks that the node should not accept any pod that do not tolerate the taints
- tolerations are applied to pds
- allows the pdos to schedule onth nodes with matching taints

** concepts
- apply taint using =$ kubectl taint nodes node1 key=value:NoSchedule=
- no pod will be able to schedule onto node1 unless it has matching toleration
- remove taint using =$ kubectl taint nodes node1 key:Noschedule-=
- specify a toleration for a pod in podSpec
- will not schedule pods that doesn't tolerate these taints
- =PreferSchedule= soft version of =NoSchedule=, system will try to avoid placing a pod that doesn't tolerate the taint, but not required
- you can put multiple taints on same node
- multiple tolerations on same pod
- if there is atleast one un-ignored taint with effect NoSchedule, pod will not be scheduled on that node
- if no un-ignored taint with effect NoSchedule, but at least on un-ignored taint with PreferNoSchedule, will try not to schedule
- if at least on un-ignored taint with effect NoExecute, then pod will be evicted from that node and will not be scheduled later

#+BEGIN_SRC bash
$ kubectl taint nodes node1 key1=value1:NoSchedule
$ kubectl taint nodes node1 key1=value1:NoExecute
$ kubectl taint nodes node1 key2=value2:NoSchedule
#+END_SRC

#+BEGIN_SRC yaml
tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"

tolerations:
- key: "key"
  operator: "Exists"
  effect: "NoSchedule"
#+END_SRC

- the pod will not be scheduled onto the node, as no tolertion matching the third taint
- but it will continue running if already running on the node when taint is added
- if a taint with NoExecute is added to the node, any pods that do not tolerate the taint will be evicted immediately
  
** example use cases
*** dedicated nodes
- =$ kubectl taint nodes nodename dedicated=groupName:NoSchedule=
- add corresponding toleration to their pods
- they will be allowd to use the tainted(dedicated) nodes as well as other nodes in the cluster
- if you want to dedicate the nodes to them and ensure they only use dedicated nodes, you should use additional label =dedicated=groupName=

*** nodes with special hardware
- in a cluster, small subet of nodes have speciliazed hardware
- it is desirable to keep pods that don't need the specialized hardware out of those nodes
- =$ kubectl taint nodes nodename special=true:NoSchedule= or =$ kubectl taitn nodes nodename spaceial=true:PreferNoSchedule=
* [[https://kubernetes.io/docs/concepts/configuration/secret/][secret]]
- intended to hold sensitive info - password, oauth token, ssh keys
- secret is more safer and flexible than putting it it pod definition or docker image

** overview
- contains small amount of sensitive data
- allows for more control over how it is used
- reduces the risk of accidental exposeure
- to use a secret, pod needs to reference the secret
- can be used with a pod in two ways
  - as file in a volume mounted on one or more containers
  - used by kubelet when pulling image for the pod

** built in secrets
- k8s automatically create secrets which contain credentials for accessing the api
- automatically modifies pods to use this type of secret

** creating your own secret
*** using kubectl
** use cases
*** pod with ssh-keys
#+BEGIN_SRC bash
$ kubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/path/to/.ssh/id_rsa --from-file=ssh-publickey=/path/to/.ssh/id_rsa.pub
#+END_SRC

#+BEGIN_SRC yaml
kind: Pod
apiVersion: v1
metadata:
  name: secret-test-pod
  labels:
    name: secret-test
spec:
  volumes:
  - name: secret-volume
    secret:
      secretName: ssh-key-secret
  containers:
  - name: ssh-test-container
    image: mySshImage
    volumeMounts:
    - name: secret-volume
      readOnly: true
      mountPath: "/etc/secret-volume"
#+END_SRC
** bset practices
- when deploying apps that interact with secrets api, should use RBAC
- watch and list requsts for all secrets in a cluster are extremely powerful capabilities
- should be avoided
- should be reserved for only the most privileged, system-level components

** security properties
*** protection
- secret can be created independently of the pods that use them
- there is less risk of secret being exposed during the workflow of creating viewing and editing pods
- secret is only sent to a node if a pod on that node requires it
- it is not written to disk
- stored in tmpfs
- deleted once the pod that depends on it is deleted
- there may be secrets for several pods on the same node
- only the secrets that a pod requires are potentially visible within its containers
- one pod doesn't have access to secrets of another pod

*** risks
- in api server, secret data is stored in plaintext in etcd
- admin should limit access to etcd to admin users
- secret data in apiserver is at rest on disk that etcd uses
- admins may want to wipe disks used by etcd when no longer in use
- if configure the secret through json/yaml, which is the secret data encoded as base64, sharing this file or checking it in to a source repository compromises the secret
- application still need to protect the value of secret after reading it from volume
- A user who can create a pod that uses a secret can also see the value of the secret
- if the apiserver doesn't allow user to read the secret object, user could run a pod which exposes the secret
- if multiple replicas of etcd are run, then secrets will be shared between them
- etcd doesn't secure peer to peer communication with ssl/tls
- anyone with root on any node can read any secret from apiserver
* [[https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/][organizing cluster access using kubeconfig file]]
- by default, kubectl looks for =$HOME/.kube/config=
- specify =KUBECONFIG= variable
- setting =--kubeconfig= flag

** supporting multiple lcuster,uers, authentication mechanism
- running kubelet might authenticate using certs
- a user might authenticate using tokens
- administrators might have sets of certificates that they provide to individual users
- using kubeconfig file, you can organize clusters, users, namespaces
- also define contexts to quickly switch between clusters and namespaces

** context
- context in kubeconfig is used to group access parametes under a convenient name
- three parametess
  - cluster
  - namespace
  - user

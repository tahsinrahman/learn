* overview
- we use kubernetes api objects to describe cluster's desired state
- what applications to run
- what container image they use
- no of raplicas
- network and disk/cpu usage

- kubernetes performs variety of tasks automatically
- starting/restarting containers
- scaling no of replicas

- kubernetes master- three processes that run on a single node
- kube-apiserver
- kube-controller-manager
- kube-scheduler

- other/worker node - two processes on each node
- kubelet: communicates with the master
- kube-proxy: proxy network that reflects networking services on each node

* kubernetes-objects
- basic objects
- pod
- service
- volume
- namespace

- high level abstractions: controllers
- based on basic objects
- provides additional functionality

- controllers
- replicaset
- deployment
- statefulset
- daemonset
- job

* kubernetes control plane
- govern how kubernetes communicates with the cluster
- maintains record of all objects
- runs continuous loop to manage those objects state
- responses to change in the claster: work to make actual state to match desired state for all objects


*** kubernetes master
- maintains desired state of the cluster

*** kubernetes nodes
- machines (vm/physical server)
- run apps
- managed my master

* [[https://kubernetes.io/docs/concepts/overview/components][kubernetes components]]
** master components
*** kube-apiserver
- exposes the kubernetes api
- front-end for kubernetes control plane

*** etcd
- key-value store: storeage for all cluster data

*** kube-scheduler
- watches newly created pods
- assigns nodes to them

*** kube-controller-manager
- runs controller(control loop: watches the state of the cluster, works to move current state to desired state)
- each controller: separate process, single binary, runs as signle process
- node controller: notice and responds when nodes go down
- replication controller: maintains the correct number of pods
- endpoints controller: populates the endpoints objects(joins services and pods)
- service account and token controller: create accounts and api access tokens for new namespace

*** cloud controller manager
- runs controller that interacts with underlying cloud providers
- controllers:
- node controllers: checks the cloud provider to determine if a node has been deleted
- router controllers: sets up routes in underlying cloud infra
- service controller: create/update/delete cloud provider node balancer
- volume controller: attach/mount/interacts with cloud provider volumes

** node components
- runs on every nodes
- maintains running pods

*** kubelet
- makes sure that containers are running in a pod
- pods are running and healthy
- doesn't manage containers

*** kube-proxy
*** container runtime
* [[https://kubernetes.io/docs/concepts/overview/kubernetes-api/][kubernetes api]]
* [[https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/][working with kubernetes objects]]
** overview
- persistent entities in the kubernetes system
- kubernetes uses these entities to represent state of the cluster
  - what apps are running, on which nodes
  - resources available to the apps
  - policies around how those apps behave(restart policies, upgrade, fault-tolerance)
- once objects are created, kubernetes works to ensure that objects remain exist

*** object spec and status
 - every object includes two nested object fields that govern the object's configuration: object spec and object status
 - object spec
   - one must provide
   - describes desired state for the object
 - object status
   - actual state of the object
   - supplied and updated by kubernetes
   - kubernetes control plane manages objects actual state to match desired state
** names
** namespaces
- kubernetes supports multiple vertical clusters backed by same physical server
- these virtual clusters are namespaces

*** when to use multiple namespaces
- many users spread across multiple teams/projects
- divide cluster resources between multiple users

*** viewing namespaces
#+BEGIN_SRC shebang
$ kubectl get namespaces
NAME          STATUS    AGE
default       Active    1d
kube-system   Active    1d
kube-public   Active    1d
#+END_SRC

- three initial namespaces
  - default: default namespaces
  - kube-system: namespace for objects created by kubernetes system
  - kube-public: readable by all users
  - 
** labels and selectors
*** labels
- key-value pairs that are attached to an object
- identify attributes of an object that are meaningful to users
- organize and select subset of objects
- labels can be attached on creation/modified later
- 

*** selectors
- many objects  have same lables
- user can identify a set of objects via label selector
- empty label selector selects every object
- null label selector selects no object
- two types of selectors: equality based, set based
  - equality based: filter by keys and values
#+BEGIN_SRC yaml
apiVersion: v1
kind: pod
metadata:
  name: cuda-test
spec:
  containers:
  - name: cuda-test
    image: "k8s.gcr.io/cuda-vector-add:v0.1"
    resources:
      limits:
        nvidia.com/gpu: 1
  nodeSelector:
    accelerator: nvidia-tesla-p100
#+END_SRC
  - set based: allows filtering keys according to a set of values
#+BEGIN_SRC
environment in (production, qa)
tier notin (frontend, backend)
partion
!partition
#+END_SRC

*** watch and list filtering
#+BEGIN_SRC shellbang
kubectl get pods -l environment=production,tier=frontend
kubectl get pods -l 'environment in (production), tier in(frontend)'
kubectl get pods -l 'environment in (production, qa)'
kubectl get pods -l 'environment,environment notin (frontend)'
#+END_SRC

#+BEGIN_SRC yaml
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}
#+END_SRC
** annotations
- non-identifying metadata
- one can use either label or annotation
- labels are used to select and find collection of objects that satisfy certain conditions
- annotations are not used to identify and select objects
- build, release, image information like timestamp, git branch, pr number, hash

** field selector
- select kubernetes objects based on value of one or more resource fields
$ kubectl get pods --field-sellector status.phase=Running
* [[https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/][kubernetes object management]]
* kubernetes architecture 
** nodes
- worker machines
- vm/physical machine
- managed by master
- runs containers
- includes
  - container runtime
  - kubelet
  - kubeproxy
  

*** node controller
- master component
- manages various aspects of nodes
- assigns CIDR block
- update list of nodes
- monitor nodes' health
- when node becomes unhealthy, it asks the cloud provider if it is still available, if not deletes it from list of nodes

** master-node communication
*** cluster to master
- communication path terminate at apiserver(other master components are not exposed)
- apiserver listens at HTTPS 443 port
- nodes contain public root certificate of the cluster
- when pods are created, 

*** master to cluster
**** apiserver to kubelet
used for:
- fetching logs for pods
- attaching to running pods
- providing kubelet's port-forwarding functionality
- 

**** apiserver to nodes/pods/services
- HTTP connection
- not authenticated/encrypted
- not confurrently safe
** cloud controller manager
- design based on plugin mechanism
- allows cloud providers integrate with kubernetes using plugins
- 

*** without cloud controller manager
#+DOWNLOADED: /tmp/screenshot.png @ 2018-11-30 12:53:16
[[file:kubernetes%20architecture/screenshot_2018-11-30_12-53-16.png]]

*** with cloud controller manager
  
#+DOWNLOADED: /tmp/screenshot.png @ 2018-11-30 12:47:37
[[file:kubernetes%20architecture/screenshot_2018-11-30_12-47-37.png]]



*** cloud controller manager runs:
- node controller
- router controller
- service controller
- persistent volume label controller
**** node controller
- initialize a node with cloud specific zone/region label
- initialize a node with cloud specifi detail, type and size
- obtain node's network adress and hotname
- check if node is deleted from cloud, if yes then delete the kubernetes node object
**** route  controllers
- configures routes so containers on different nodes in the cluster can communicate with each other
- only applicable for GCE clusters
**** service controller
- listens to service create, update, delete events
- configures cloud load balancers
- 

**** persistent volume labels controller
- applies labels on volumes when crated
- removes the need of users manually set the labels
- these labels are essential for scheduling pods, as they only work within the region/zone that they are in, so pods need to be in same region
- 

** kubelet
- node controller contains cloud dependent functionality of the kubelet
- before, kubelet was responsible for cloud-specific details(ip address, region/zone labels, instant type information)
- now, the initialization operations are done in ccm
- now, kubelet initialize a node without cloud specific information
- new nodes are unschedulable until ccm initializes the node with cloud specific informations
* [[https://kubernetes.io/docs/concepts/containers/][containers]]
* [[https://kubernetes.io/docs/concepts/workloads/pods/][pods]]
** overview
*** understanding pods
**** overview
     - smallest deployable object
     - basic building block
     - smallest and simplest unit
     - represents a running process
     - pods encapsulate
       - application containers(or multiple containers)
       - storage
       - unique ip
       - options that govern how the containner(s) should run
     - containers are tightly coupled and share resource
     - single instance of an application
       - if scaled horizontally, multiple pods, one for each instance
       - replicated pods created and managed by controller

**** pods that run single containers
     - most common use case
     - pod as a wrapper around single container
     - kubernetes manages pod rather than conntainers directly

**** pods that run multiple containers
     - encapsulate an application that composed of multiple co-located containers that are tightly coupled and need to share resource
     - form a single cohesive unit of service

**** how pods manage multiple containers
     - multiple cooperating processes as containers form a cohesive unit of service
     - containers are automatically co-located and co-scheduled on same node
     - share resource, dependencies
     - communicate with each other
     - coordinate when and how to terminate
     - advanced use case
     - only use when containers are tightly coupled

**** shared networking
     - each pod is assigned a unique ip address
     - every container in a pod shares the network namespace including ip addresses and ports
     - containers can communicate with each other using *localhost*

**** shared storage
     - pod specify a set of shared storage volumes
     - all containers can access the shared volumes
     - allows persistent data in a pod
*** working with pods
    - you'll rarely create individual pods directly
      - pods are designed as relatively short lived, disposable entities
      - when created, it is scheduled to run on a node
      - remains on that node until process terminates, then pod is deleted
      - do not self-heal
      - pods get deleted
        - if node fails
        - scheduling operations fails
        - lack of resource
        - node maintainance
    - controller handles the work managing the disposable pod instances
    - so, it is far more common to manage pods using a controller
*** pods and controllers
    - create and manage multiple pods
    - handles replication, rollout and self healing capabilities
    - if node fails, controller might automatically replace the pod by scheduling an identical replacement on different node
    - example of controllers that contain one or more pors
      - deployment
      - statefulset
      - daemonset
*** pod template
    - pod specifications which are included in other objects such as replication controller, jobs, daemonset
    - uses pod templates to make actual pods

    #+BEGIN_SRC yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: myapp-container
      image: busybox
      command: ['sh', '-c', 'echo hello kubernetes! && sleep 3600']
    #+END_SRC

    - after creation, pods can't be updated
    - new template has no direct effect on pods already created
    - but pods created by controllers can be updated directly
** pods
   - pods are isolated by linux namespaces, cgroups
   - within pod, individual containers are furher sub-isolationed
   - containers within a pod share an ip addresses and port space
   - find each other via localhost
   - communicate with each other using standard inter process communications like semaphores or shared memory
   - containers in different pods have different ip addresses
   - can't communicate by IPC
   - they communicate via pod ip address
   - 

** termination of pods
   - pods are gracefully terminated
   - when users request deleteion of pod, system records the intended grace period and TERM signal is sent to main process of each container
   - once grace period has expired, the KILL signal is send to those processes
   - pods are then deleted from API server
** container probes
- probe is a diagnostic performed periodically by kubelet
- kubelet calls a handler implemented by the container
- 3 types of handlers
  - runs specific command inside the container. diagnostic is successful if command exists with status code 0
  - performs TCP check against container's IP on spefcified port. diagnostic successful if port is open
  - performs HTTP GET request against container's IP address on specified port and path. Diagnostic successful if response code betwen [200, 400)
- optionally performs two types of probles
  - livenessProbe:
    - indicates whether the container is running
    - if fails, kubelet kills the container
    - container is then subjected to its restart policy
    - if container does not provide a liveness probe, default state is success
  - readinessProbe:
    - indicates whether the container is ready to service requests
    - if fails, the endpoint controller removes pod's ip address from all services that match the pod
    - if container does not provide a readinessProbe, default state is success
    
*** when to use probes
- if container crashes on its own whenever it encounters an issue or becomes unhealty, no need for liveness probe
- if you want to send traffic to pod when a probe succeds, specify readiness probe
- the pod will start without recieving any traffic, only start recieiving after probe starts succeeding
- if container needs to load large data, configuring files, specify readiness probe
** init containers
** pod presets
*** overview
- inject additional runtime requirements into a pod at creation time
- use label selectors to specify the pods to which the preset applies
- allows not to have explicitly provide all information for every pod
- authors of pod templates consuming a specific service do not need to konow all the details of that service

*** how it works
- admission controller(PodPreset) which when enabled, applies pod presets to incoming pod creation request
- when pod creation request occures, system does
  - retrieve all PodPrestets
  - checks if label selectors of any PodPreset matches the labels of the pod
  - attempts to merge various resources defined by podpreset
  - on error, through event, and create the pod without injecting any resource from podpreset
  - annotate the the resulting modified pod spec to indicate that it has been modified by podpreset
- when podpreset applies to one or more pods
  - modifies the pod specs
  - for Env EnvFrom VolumeMounts, modifies the container spec for all containers
  - for Volume, modifies the pod spec
** discuptons

* [[https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/][replica set]]
- next generation replication controller
- only difference between replicaset and replication controller is selector support
- replicaset supppports the new set based selector
- replication controller only supports equality based selector


** how to use replicaset
- most kubectl commands that supports replication controller also supports replicasets
- only exception is rolling-update command
- for rolling update functionality, one should use deployments instead
- replicasets can be used independently
- mainly used by deployments


** when to use replicaset
- replicaset ensures that spcified number of pod replicas are running at any given time
- we should use deployments instead of directly using replicasets
* [[https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/][replicationcontroller]]
- ensures that specified number of pod replicas are running at any one time
- makes sure that pods are always up and available

** how replicationcontroller works
- if too many pods, terminates extra pods
- if too few pods, creates more pods
- pods managed by replicationcontroller are automatically replaced if they fail, deleted or terminated
* [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/][deployments]]
- controller that provides declarative udpates for pods and replicasets
- you describe a desired state in deployment, and deployment controller changes tha actual state to the desired 
* [[https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/][statefulset]]
- used to manage stateful applications
- manages the deployment and scaling of a set of pods
- provides gurantee about ordering and uniquness of these pods
- like deployment, manages pods that are based on an identical container spec
- unlike deployments, maintains a sticky identity for each of their pods
- these pods are of same specs, but are not interchangeable
- each has a persistent identifier
- that is maintained across any rescheduling


** using statefulsets
- stable unique network identifier
- stable, persistent storage
- ordered, graceful deployment and scaling
- ordered, automated rolling updates
- if application doesn't require any stable identifiers or ordered deployment, deletion or scaling, you should deploy it with deployments

** limitaions
- deleting/scaling down doesn't delete the volumes associated with the stateful
- it is because of data safety
- do not guarantee on the termination of pods when statefulset is deleted
- to achive ordered and graceful termination of the pods, it is possible ot scale down to 0 prior to deletion
